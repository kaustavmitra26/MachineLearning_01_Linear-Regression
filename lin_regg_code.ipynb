#import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#import dataset 
data=pd.read_csv("Data.csv")
data.head()

X=data.iloc[ : , :-1].values #Except last column
Y=data.iloc[ : ,-1].values #Only last column

X[1]
Y

#filling missing values in data using 'imputation method ' here 'mean_method of imputation is used' 
from sklearn.preprocessing import Imputer
imputer=Imputer(missing_values='NaN',strategy='mean',axis=0)
imputer=imputer.fit(X[:,1:])
X[:,1:]=imputer.transform(X[:,1:])

X

#labelling the data using 'LabelEncoder'
from sklearn.preprocessing import LabelEncoder
labelencoder_X=LabelEncoder()
X[:,0]=labelencoder_X.fit_transform(X[:,0])
X

from sklearn.preprocessing import OneHotEncoder
onehotencoder=OneHotEncoder(categorical_features=[0])
X=onehotencoder.fit_transform(X).toarray()
labelencoder_y=LabelEncoder()
Y=labelencoder_y.fit_transform(Y)

X

#train test split
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)

#standard scalar is used for applying 'central limit theorem' i.e to make our data normaly distributed
from sklearn.preprocessing import StandardScaler
sc_x=StandardScaler()
x_train=sc_x.fit_transform(x_train)
x_test=sc_x.transform(x_test)
data1=pd.read_csv("Salary_Data.csv")
X1=data1.iloc[:,:-1].values
Y1=data1.iloc[:,1].values
x1_train,x1_test,y1_train,y1_test=train_test_split(X1,Y1,test_size=0.2,random_state=42)

#fit data in linear regression
from sklearn.linear_model import LinearRegression
regressor= LinearRegression()
regressor.fit(x1_train,y1_train)
y1_pred=regressor.predict(x1_test)

#visualisation
plt.scatter(x1_train,y1_train, color='red')
plt.plot(x1_train, regressor.predict(x1_train),color='blue')
plt.show()

plt.scatter(x1_test,y1_test, color='red')
plt.plot(x1_test,y1_pred,color='blue')
plt.show()


